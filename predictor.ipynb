{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef54544",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e769d7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/shubhdeepdas/Desktop/Code/Research_Stuff/Disease Predictor/Final_Augmented_dataset_Diseases_and_Symptoms.csv')\n",
    "\n",
    "print(\"Basic info:\")\n",
    "print(df.info())\n",
    "\n",
    "# 2. List all column names\n",
    "print(\"\\nColumns:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# 3. Summary statistics of numeric columns\n",
    "print(\"\\nSummary statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "# 4. Number of unique values in each column (to identify categorical columns)\n",
    "print(\"\\nUnique values count per column:\")\n",
    "print(df.nunique())\n",
    "\n",
    "# 5. Sample unique values from 'diseases' column\n",
    "print(\"\\nUnique values in 'diseases' column:\")\n",
    "print(df['diseases'].unique())\n",
    "\n",
    "# 6. Data types of all columns\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# 7. Check for any missing values in columns\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# 8. Count of 1s (true values) in symptom columns\n",
    "symptom_cols = df.columns.difference(['diseases'])\n",
    "print(\"\\nCount of true (1) values per symptom column:\")\n",
    "print(df[symptom_cols].sum())\n",
    "\n",
    "# 9. Row with highest number of symptoms true (after ensuring numeric conversion)\n",
    "df[symptom_cols] = df[symptom_cols].apply(pd.to_numeric, errors='coerce').fillna(0).astype(int)\n",
    "df['symptom_count'] = df[symptom_cols].sum(axis=1)\n",
    "max_symptoms_row = df.loc[df['symptom_count'].idxmax()]\n",
    "print(\"\\nRow with highest number of symptoms true:\")\n",
    "print(max_symptoms_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94e2947",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in df['diseases'].to_list():\n",
    "    if 'arthritis' in i:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343b1cc0",
   "metadata": {},
   "source": [
    "# Run the Code below to Generate the Csv of symptoms, description and synonyms (No need to run if you already have db but if you unsatisfied with the Current one use your own model to generate it again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf4aa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code was to make the symptoms list better for RAG Search \n",
    "\n",
    "\n",
    "\n",
    "# --------- CONFIG ---------\n",
    "OLLAMA_URL = \"http://localhost:11434/api/generate\"\n",
    "MODEL_NAME = \"gemma3:1b\"   # or 'deepseek-r1:1.5b', etc.\n",
    "OUTPUT_CSV = \"symptom_metadata.csv\"\n",
    "SLEEP_BETWEEN_CALLS = 0.2   # seconds\n",
    "MAX_RETRIES_PER_SYMPTOM = 2\n",
    "# --------------------------\n",
    "\n",
    "\n",
    "# --- STEP 0: Get symptom list from your dataframe ---\n",
    "\n",
    "clmn = list(df.columns.unique())\n",
    "symptms = clmn[1:]  # assuming first column is not a symptom\n",
    "print(\"Found\", len(symptms), \"symptoms\")\n",
    "\n",
    "\n",
    "# --- PROMPT ---\n",
    "\n",
    "def build_prompt(symptom: str) -> str:\n",
    "    return f\"\"\"\n",
    "You are helping to standardize symptom names for a medical questionnaire.\n",
    "\n",
    "SYMPTOM: \"{symptom}\"\n",
    "\n",
    "Your task:\n",
    "Return a JSON object with EXACTLY these keys:\n",
    "- \"symptom\": the original symptom name as given.\n",
    "- \"short_description\": one short, plain-language description of what the person is feeling in their body or mind no daignosis just description (15–30 words).\n",
    "- \"synonyms\": a list of 6–12 natural phrases that real patients might say when describing the SAME symptom in everyday language.\n",
    "\n",
    "Very important rules:\n",
    "- NEVER leave \"short_description\" or \"synonyms\" empty.\n",
    "- ALWAYS produce at least 6 synonyms. Prefer 8–10 when possible.\n",
    "- Synonyms MUST be full phrases, not single words. For example:\n",
    "  - GOOD: \"I feel short of breath\", \"I can't catch my breath\", \"I get tired after walking a little\"\n",
    "  - BAD: \"breathlessness\", \"dyspnea\", \"unease\"\n",
    "- Avoid very technical words unless people commonly say them.\n",
    "- Do NOT diagnose. Just describe what the person feels (e.g., \"feeling very sad and hopeless\" instead of \"major depressive disorder\").\n",
    "- Use simple English, neutral tone, not dramatic or scary.\n",
    "- Always talk in third person or generic style (e.g., \"feeling…\" or \"I feel…\") – like a patient describing their problem.\n",
    "\n",
    "Output format:\n",
    "Return ONLY a valid JSON object, nothing else. Example format (structure only):\n",
    "\n",
    "{{\n",
    "  \"symptom\": \"anxiety and nervousness\",\n",
    "  \"short_description\": \"A state of feeling tense, on edge, or worried, often with physical signs like a racing heart or restlessness.\",\n",
    "  \"synonyms\": [\n",
    "    \"I feel very anxious and restless\",\n",
    "    \"I can't stop worrying about everything\",\n",
    "    \"my heart keeps racing and I feel on edge\",\n",
    "    \"I feel nervous for no clear reason\",\n",
    "    \"I feel shaky and tense all the time\",\n",
    "    \"I can't relax, my mind is always racing\"\n",
    "  ]\n",
    "}}\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "# --- Call Ollama and parse JSON with basic quality check ---\n",
    "\n",
    "def _single_call(symptom: str) -> dict:\n",
    "    payload = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"prompt\": build_prompt(symptom),\n",
    "        \"format\": \"json\",\n",
    "        \"stream\": False,\n",
    "    }\n",
    "\n",
    "    resp = requests.post(OLLAMA_URL, json=payload, timeout=180)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "\n",
    "    raw = data.get(\"response\", \"\")\n",
    "\n",
    "    try:\n",
    "        parsed = json.loads(raw)\n",
    "    except json.JSONDecodeError:\n",
    "        # Try to rescue a JSON substring\n",
    "        start = raw.find(\"{\")\n",
    "        end = raw.rfind(\"}\")\n",
    "        if start != -1 and end != -1:\n",
    "            parsed = json.loads(raw[start:end + 1])\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "    # Defaults & normalization\n",
    "    parsed.setdefault(\"symptom\", symptom)\n",
    "    parsed.setdefault(\"short_description\", \"\")\n",
    "    parsed.setdefault(\"synonyms\", [])\n",
    "\n",
    "    if not isinstance(parsed[\"synonyms\"], list):\n",
    "        parsed[\"synonyms\"] = [str(parsed[\"synonyms\"])]\n",
    "\n",
    "    parsed[\"synonyms\"] = [\n",
    "        str(s).strip() for s in parsed[\"synonyms\"] if str(s).strip()\n",
    "    ]\n",
    "\n",
    "    return parsed\n",
    "\n",
    "\n",
    "def _is_good_enough(meta: dict) -> bool:\n",
    "    desc = str(meta.get(\"short_description\", \"\")).strip()\n",
    "    syns = meta.get(\"synonyms\", [])\n",
    "    if not isinstance(syns, list):\n",
    "        syns = [syns]\n",
    "    syns = [str(s).strip() for s in syns if str(s).strip()]\n",
    "\n",
    "    # basic quality criteria – tune as you like\n",
    "    if len(desc.split()) < 5:  # too short / empty\n",
    "        return False\n",
    "    if len(syns) < 4:  # too few synonyms\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def query_ollama_with_retry(symptom: str, max_retries: int = MAX_RETRIES_PER_SYMPTOM) -> dict:\n",
    "    last_meta = None\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            meta = _single_call(symptom)\n",
    "            last_meta = meta\n",
    "            if _is_good_enough(meta):\n",
    "                return meta\n",
    "            else:\n",
    "                print(f\"    -> Output not good enough (attempt {attempt}). Retrying...\")\n",
    "        except Exception as e:\n",
    "            print(f\"    -> Error calling model on attempt {attempt}: {e}\")\n",
    "        time.sleep(0.5)  # small delay before retry\n",
    "\n",
    "    # If we reach here, either all attempts failed or never met quality criteria\n",
    "    print(\"    -> Using last available output (may be incomplete).\")\n",
    "    if last_meta is None:\n",
    "        last_meta = {\n",
    "            \"symptom\": symptom,\n",
    "            \"short_description\": \"\",\n",
    "            \"synonyms\": [],\n",
    "        }\n",
    "    return last_meta\n",
    "\n",
    "\n",
    "# --- Load already processed symptoms if CSV exists ---\n",
    "\n",
    "processed_symptoms = set()\n",
    "header_exists = os.path.exists(OUTPUT_CSV)\n",
    "\n",
    "if header_exists:\n",
    "    try:\n",
    "        existing_df = pd.read_csv(OUTPUT_CSV)\n",
    "        if \"symptom\" in existing_df.columns:\n",
    "            processed_symptoms = set(existing_df[\"symptom\"].astype(str).tolist())\n",
    "            print(f\"Resuming: found {len(processed_symptoms)} already processed symptoms.\")\n",
    "        else:\n",
    "            print(f\"Warning: {OUTPUT_CSV} exists but has no 'symptom' column. Treating as empty.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not read existing {OUTPUT_CSV}: {e}\")\n",
    "        print(\"Continuing as if no previous file exists.\")\n",
    "        header_exists = False\n",
    "        processed_symptoms = set()\n",
    "else:\n",
    "    print(\"No existing CSV found. Starting fresh.\")\n",
    "\n",
    "header_written = header_exists\n",
    "\n",
    "# --- Loop, query, and APPEND row-by-row ---\n",
    "\n",
    "for idx, s in enumerate(symptms, start=1):\n",
    "    s_str = str(s)\n",
    "\n",
    "    if s_str in processed_symptoms:\n",
    "        print(f\"[{idx}/{len(symptms)}] Skipping already processed symptom: {s_str!r}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"[{idx}/{len(symptms)}] Generating metadata for symptom: {s_str!r}\")\n",
    "\n",
    "    meta = query_ollama_with_retry(s_str)\n",
    "    desc = str(meta.get(\"short_description\", \"\")).strip()\n",
    "    syns_list = meta.get(\"synonyms\", [])\n",
    "    if not isinstance(syns_list, list):\n",
    "        syns_list = [syns_list]\n",
    "    syns_list = [str(x).strip() for x in syns_list if str(x).strip()]\n",
    "\n",
    "    row_dict = {\n",
    "        \"symptom\": meta.get(\"symptom\", s_str),\n",
    "        \"short_description\": desc,\n",
    "        # keep separator; you can change to \"|\" if you want\n",
    "        \"synonyms\": \"; \".join(syns_list),\n",
    "    }\n",
    "\n",
    "    row_df = pd.DataFrame([row_dict])\n",
    "\n",
    "    row_df.to_csv(\n",
    "        OUTPUT_CSV,\n",
    "        mode=\"a\",\n",
    "        index=False,\n",
    "        header=not header_written,\n",
    "    )\n",
    "\n",
    "    header_written = True\n",
    "    processed_symptoms.add(s_str)\n",
    "\n",
    "    time.sleep(SLEEP_BETWEEN_CALLS)\n",
    "\n",
    "print(\"Done. Incremental results saved in:\", OUTPUT_CSV)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd25b17c",
   "metadata": {},
   "source": [
    "# This generates a vector db using three diffrent tokenizers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b8d9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build_symptom_indices.py\n",
    "\n",
    "\n",
    "# === CONFIG ===\n",
    "CSV_PATH = \"/Users/shubhdeepdas/Desktop/Code/Research_Stuff/Disease Predictor/symptom_metadata.csv\"\n",
    "BASE_OUTPUT_DIR = Path(\"/Users/shubhdeepdas/Desktop/Code/Research_Stuff/Disease Predictor/symptom_index\")\n",
    "\n",
    "# Model configs: key = short name, value = HF model id\n",
    "MODEL_CONFIGS = {\n",
    "    \"minilm\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    \"mpnet\": \"sentence-transformers/all-mpnet-base-v2\",\n",
    "    \"biolord\": \"FremyCompany/BioLORD-2023\",  # e.g. \"pritamdeka/BioSimCSE-BioClinicalBERT\" or your BioLORD id\n",
    "}\n",
    "# ==============\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "\n",
    "def build_symptom_docs(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Build rich text docs per symptom:\n",
    "    - name\n",
    "    - short_description\n",
    "    - synonyms (split by ';')\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "    for _, row in df.iterrows():\n",
    "        name = str(row[\"symptom\"]).strip()\n",
    "        desc = str(row.get(\"short_description\", \"\")).strip()\n",
    "        syns = str(row.get(\"synonyms\", \"\")).strip()\n",
    "\n",
    "        syn_list = [s.strip() for s in syns.split(\";\") if s.strip()]\n",
    "        syn_text = \". \".join(syn_list)\n",
    "\n",
    "        text = f\"{name}.\"\n",
    "        if desc:\n",
    "            text += f\" {desc}\"\n",
    "        if syn_text:\n",
    "            text += f\" Synonyms/phrases: {syn_text}\"\n",
    "\n",
    "        docs.append({\"name\": name, \"text\": text})\n",
    "    return docs\n",
    "\n",
    "\n",
    "def build_and_save_index(model_key: str, model_name: str, docs: list[dict]):\n",
    "    print(f\"\\n=== Building index for [{model_key}] -> {model_name} ===\")\n",
    "\n",
    "    # Prepare output dir for this model\n",
    "    output_dir = BASE_OUTPUT_DIR / model_key\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Extract texts/names\n",
    "    symptom_texts = [d[\"text\"] for d in docs]\n",
    "    symptom_names = [d[\"name\"] for d in docs]\n",
    "\n",
    "    print(f\"Encoding {len(symptom_texts)} symptom docs with {model_name} ...\")\n",
    "    model = SentenceTransformer(model_name)\n",
    "\n",
    "    symptom_embs = model.encode(symptom_texts, normalize_embeddings=True)\n",
    "    symptom_embs = np.array(symptom_embs)\n",
    "\n",
    "    print(\"Embeddings shape:\", symptom_embs.shape)\n",
    "\n",
    "    # Save embeddings + meta\n",
    "    emb_path = output_dir / \"symptom_embs.npy\"\n",
    "    meta_path = output_dir / \"symptom_meta.json\"\n",
    "\n",
    "    np.save(emb_path, symptom_embs)\n",
    "\n",
    "    with open(meta_path, \"w\") as f:\n",
    "        json.dump(\n",
    "            {\n",
    "                \"model_name\": model_name,\n",
    "                \"symptom_names\": symptom_names,\n",
    "                \"symptom_texts\": symptom_texts,\n",
    "            },\n",
    "            f,\n",
    "            indent=2,\n",
    "        )\n",
    "\n",
    "    print(f\"Saved embeddings to {emb_path}\")\n",
    "    print(f\"Saved metadata to {meta_path}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    symptom_docs = build_symptom_docs(df)\n",
    "    print(f\"Built {len(symptom_docs)} symptom docs from CSV.\")\n",
    "\n",
    "    for key, model_name in MODEL_CONFIGS.items():\n",
    "        if \"REPLACE_WITH_BIOLORD_MODEL_NAME\" in model_name:\n",
    "            print(\n",
    "                f\"\\n[WARN] Skipping model '{key}' because it still has the placeholder name. \"\n",
    "                \"Edit MODEL_CONFIGS to set the actual BioLORD model id.\"\n",
    "            )\n",
    "            continue\n",
    "        build_and_save_index(key, model_name, symptom_docs)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc2d51e",
   "metadata": {},
   "source": [
    "# This Compares all three embedding models to accuracy and retrival quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b79d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare_embedding_models.py\n",
    "\n",
    "\n",
    "\n",
    "BASE_INDEX_DIR = Path(\"/Users/shubhdeepdas/Desktop/Code/Research_Stuff/Disease Predictor/symptom_index\")\n",
    "\n",
    "# Must match the folder names used in build_symptom_indices.py\n",
    "MODEL_KEYS = [\"minilm\", \"mpnet\", \"biolord\"]\n",
    "\n",
    "# Some example \"patient sentences\" to test\n",
    "TEST_QUERIES = [\n",
    "    \"my stomach has been hurting badly since yesterday\",\n",
    "    \"I feel really dizzy and the room spins when I stand up\",\n",
    "    \"I can't stop worrying and my heart keeps racing all the time\",\n",
    "]\n",
    "\n",
    "\n",
    "def load_index(model_key: str):\n",
    "    \"\"\"\n",
    "    Load embeddings + meta for a given model_key (minilm/mpnet/biolord).\n",
    "    \"\"\"\n",
    "    index_dir = BASE_INDEX_DIR / model_key\n",
    "    emb_path = index_dir / \"symptom_embs.npy\"\n",
    "    meta_path = index_dir / \"symptom_meta.json\"\n",
    "\n",
    "    if not emb_path.exists() or not meta_path.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Index files not found for model '{model_key}' in {index_dir}. \"\n",
    "            \"Did you run build_symptom_indices.py and configure the model names correctly?\"\n",
    "        )\n",
    "\n",
    "    symptom_embs = np.load(emb_path)\n",
    "    with open(meta_path) as f:\n",
    "        meta = json.load(f)\n",
    "\n",
    "    symptom_names = meta[\"symptom_names\"]\n",
    "    symptom_texts = meta[\"symptom_texts\"]\n",
    "    model_name = meta[\"model_name\"]\n",
    "\n",
    "    return {\n",
    "        \"model_key\": model_key,\n",
    "        \"model_name\": model_name,\n",
    "        \"symptom_embs\": symptom_embs,\n",
    "        \"symptom_names\": symptom_names,\n",
    "        \"symptom_texts\": symptom_texts,\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_query_on_model(query: str, index: dict, top_k: int = 12):\n",
    "    \"\"\"\n",
    "    For one query and one model, return a list of dicts:\n",
    "    [\n",
    "      {\"query\": ..., \"model_key\": ..., \"model_name\": ..., \"rank\": 1, \"symptom\": ..., \"score\": ...},\n",
    "      ...\n",
    "    ]\n",
    "    \"\"\"\n",
    "    model_name = index[\"model_name\"]\n",
    "    model_key = index[\"model_key\"]\n",
    "    symptom_embs = index[\"symptom_embs\"]\n",
    "    symptom_names = index[\"symptom_names\"]\n",
    "\n",
    "    model = SentenceTransformer(model_name)\n",
    "\n",
    "    q_emb = model.encode([query], normalize_embeddings=True)\n",
    "    sims = util.cos_sim(q_emb, symptom_embs).cpu().numpy()[0]\n",
    "\n",
    "    top_idx = sims.argsort()[::-1][:top_k]\n",
    "\n",
    "    results = []\n",
    "    for rank, idx in enumerate(top_idx, start=1):\n",
    "        results.append(\n",
    "            {\n",
    "                \"query\": query,\n",
    "                \"model_key\": model_key,\n",
    "                \"model_name\": model_name,\n",
    "                \"rank\": rank,\n",
    "                \"symptom\": symptom_names[idx],\n",
    "                \"score\": float(sims[idx]),\n",
    "            }\n",
    "        )\n",
    "    return results\n",
    "\n",
    "\n",
    "def plot_query_results(query: str, per_model_rows: dict, top_k: int = 12):\n",
    "    \"\"\"\n",
    "    per_model_rows: { model_key: [rows from evaluate_query_on_model] }\n",
    "    Creates a matplotlib figure with one subplot per model:\n",
    "    x-axis = symptom (top-k), y-axis = similarity score.\n",
    "    \"\"\"\n",
    "    num_models = len(per_model_rows)\n",
    "    if num_models == 0:\n",
    "        return\n",
    "\n",
    "    fig, axes = plt.subplots(1, num_models, figsize=(5 * num_models, 5), squeeze=False)\n",
    "    fig.suptitle(f'Query: \"{query}\"', fontsize=14)\n",
    "\n",
    "    for col_idx, (model_key, rows) in enumerate(per_model_rows.items()):\n",
    "        ax = axes[0, col_idx]\n",
    "        if not rows:\n",
    "            ax.set_title(f\"{model_key} (no results)\")\n",
    "            ax.axis(\"off\")\n",
    "            continue\n",
    "\n",
    "        df = pd.DataFrame(rows).sort_values(by=\"rank\")\n",
    "        # Truncate long symptom names for display, but keep actual label as full on x-axis if you want\n",
    "        x_labels = df[\"symptom\"].tolist()\n",
    "        y_scores = df[\"score\"].tolist()\n",
    "\n",
    "        x_pos = np.arange(len(x_labels))\n",
    "        ax.bar(x_pos, y_scores)\n",
    "        ax.set_ylim(0, 1.0)\n",
    "        ax.set_xticks(x_pos)\n",
    "        ax.set_xticklabels(x_labels, rotation=45, ha=\"right\", fontsize=8)\n",
    "        model_name_short = df[\"model_key\"].iloc[0]\n",
    "        ax.set_title(model_name_short)\n",
    "        ax.set_ylabel(\"Similarity score\")\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.90])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Load all available indices\n",
    "    indices = {}\n",
    "    for key in MODEL_KEYS:\n",
    "        try:\n",
    "            idx = load_index(key)\n",
    "            indices[key] = idx\n",
    "            print(f\"Loaded index for model '{key}': {idx['model_name']}\")\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"[WARN] {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Failed to load model '{key}': {e}\")\n",
    "\n",
    "    if not indices:\n",
    "        print(\"No models loaded. Fix the warnings above and rerun.\")\n",
    "        return\n",
    "\n",
    "    all_rows = []\n",
    "\n",
    "    for query in TEST_QUERIES:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"QUERY: {query}\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        per_model_rows = {}\n",
    "\n",
    "        for key, idx in indices.items():\n",
    "            rows = evaluate_query_on_model(query, idx, top_k=12)\n",
    "            per_model_rows[key] = rows\n",
    "            all_rows.extend(rows)\n",
    "\n",
    "            df = pd.DataFrame(rows)\n",
    "            display_df = df[[\"model_key\", \"rank\", \"symptom\", \"score\"]]\n",
    "            print(f\"\\nTop matches for model [{key}] ({idx['model_name']}):\")\n",
    "            print(display_df.to_string(index=False))\n",
    "\n",
    "        # Plot a figure comparing all models for this query\n",
    "        plot_query_results(query, per_model_rows, top_k=12)\n",
    "\n",
    "    # Optional: combined big table for documentation\n",
    "    print(\"\\n\" + \"#\" * 80)\n",
    "    print(\"COMBINED RESULTS (all queries, all models)\")\n",
    "    print(\"#\" * 80)\n",
    "    combined_df = pd.DataFrame(all_rows)\n",
    "    combined_df = combined_df[[\"query\", \"model_key\", \"rank\", \"symptom\", \"score\"]]\n",
    "    combined_df = combined_df.sort_values(by=[\"query\", \"model_key\", \"rank\"])\n",
    "    print(combined_df.to_string(index=False))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754dbd86",
   "metadata": {},
   "source": [
    "# Block below actually does the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ae2962",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from IPython.display import display\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# ===================== CONFIG =====================\n",
    "\n",
    "# Base project dir\n",
    "BASE_DIR = Path(\"/Users/shubhdeepdas/Desktop/Code/Research_Stuff/Disease Predictor\")\n",
    "\n",
    "# Symptom CSV and index\n",
    "CSV_PATH = BASE_DIR / \"symptom_metadata.csv\"\n",
    "INDEX_DIR = BASE_DIR / \"symptom_index\" / \"minilm\"  # using MiniLM index\n",
    "\n",
    "# Conversation JSON\n",
    "CONV_PATH = BASE_DIR / \"conversations\" / \"conv-1.json\"\n",
    "\n",
    "# Disease–symptom matrix CSV\n",
    "DISEASE_CSV_PATH = (\n",
    "    BASE_DIR\n",
    "    / \"Final_Augmented_dataset_Diseases_and_Symptoms.csv\"\n",
    ")\n",
    "\n",
    "# Where to save results (CSVs & logs)\n",
    "RESULTS_DIR = BASE_DIR / \"results\"\n",
    "\n",
    "# Ollama config\n",
    "OLLAMA_URL = \"http://localhost:11434/api/generate\"\n",
    "OLLAMA_MODEL = \"qwen2.5:7b-instruct\"  # or your chosen local model\n",
    "\n",
    "TOP_K_PER_TURN = 5\n",
    "MAX_CANDIDATES = 60\n",
    "\n",
    "# Disease column name\n",
    "DISEASE_COL = \"diseases\"\n",
    "\n",
    "# ==================================================\n",
    "# SYMPTOM / CONVERSATION HELPERS\n",
    "# ==================================================\n",
    "\n",
    "\n",
    "def load_symptom_index(index_dir: Path) -> Dict[str, Any]:\n",
    "    emb_path = index_dir / \"symptom_embs.npy\"\n",
    "    meta_path = index_dir / \"symptom_meta.json\"\n",
    "\n",
    "    symptom_embs = np.load(emb_path)\n",
    "    with open(meta_path) as f:\n",
    "        meta = json.load(f)\n",
    "\n",
    "    symptom_names = meta[\"symptom_names\"]\n",
    "    symptom_texts = meta[\"symptom_texts\"]\n",
    "    model_name = meta[\"model_name\"]\n",
    "\n",
    "    model = SentenceTransformer(model_name)\n",
    "\n",
    "    return {\n",
    "        \"embs\": symptom_embs,\n",
    "        \"names\": symptom_names,\n",
    "        \"texts\": symptom_texts,\n",
    "        \"model\": model,\n",
    "        \"model_name\": model_name,\n",
    "    }\n",
    "\n",
    "\n",
    "def load_symptom_descriptions(csv_path: Path) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Map symptom name -> short_description from your CSV.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    desc_map: Dict[str, str] = {}\n",
    "    for _, row in df.iterrows():\n",
    "        name = str(row[\"symptom\"]).strip()\n",
    "        desc = str(row.get(\"short_description\", \"\")).strip()\n",
    "        desc_map[name] = desc\n",
    "    return desc_map\n",
    "\n",
    "\n",
    "def load_conversation(conv_path: Path) -> List[Dict[str, Any]]:\n",
    "    with open(conv_path) as f:\n",
    "        data = json.load(f)\n",
    "    return data  # [{speaker, message}, ...]\n",
    "\n",
    "\n",
    "def build_turn_text(turn: Dict[str, Any]) -> str:\n",
    "    return f'{turn[\"speaker\"].upper()}: {turn[\"message\"].strip()}'\n",
    "\n",
    "\n",
    "def retrieve_top_symptoms_for_text(\n",
    "    text: str,\n",
    "    model: SentenceTransformer,\n",
    "    symptom_embs: np.ndarray,\n",
    "    symptom_names: List[str],\n",
    "    top_k: int = 5,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Return top_k symptoms for this text: [{symptom, score}, ...]\n",
    "    \"\"\"\n",
    "    q_emb = model.encode([text], normalize_embeddings=True)\n",
    "    sims = util.cos_sim(q_emb, symptom_embs).cpu().numpy()[0]\n",
    "    top_idx = sims.argsort()[::-1][:top_k]\n",
    "\n",
    "    results: List[Dict[str, Any]] = []\n",
    "    for idx in top_idx:\n",
    "        results.append(\n",
    "            {\n",
    "                \"symptom\": symptom_names[idx],\n",
    "                \"score\": float(sims[idx]),\n",
    "            }\n",
    "        )\n",
    "    return results\n",
    "\n",
    "\n",
    "def build_turns_dataframe(\n",
    "    conversation: List[Dict[str, Any]],\n",
    "    index: Dict[str, Any],\n",
    "    top_k: int = 5,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each turn in the conversation, retrieve top-k symptoms and build a DataFrame.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    symptom_embs = index[\"embs\"]\n",
    "    symptom_names = index[\"names\"]\n",
    "    model = index[\"model\"]\n",
    "\n",
    "    for i, turn in enumerate(conversation):\n",
    "        text = build_turn_text(turn)\n",
    "        top_symptoms = retrieve_top_symptoms_for_text(\n",
    "            text, model, symptom_embs, symptom_names, top_k=top_k\n",
    "        )\n",
    "        rows.append(\n",
    "            {\n",
    "                \"turn_idx\": i,\n",
    "                \"speaker\": turn[\"speaker\"],\n",
    "                \"message\": turn[\"message\"],\n",
    "                \"top_symptoms\": [t[\"symptom\"] for t in top_symptoms],\n",
    "                \"scores\": [t[\"score\"] for t in top_symptoms],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_conversation_block(conversation: List[Dict[str, Any]]) -> str:\n",
    "    lines = [build_turn_text(t) for t in conversation]\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "def call_ollama(prompt: str, model: str = OLLAMA_MODEL) -> str:\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "    }\n",
    "    resp = requests.post(OLLAMA_URL, json=payload)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "    return data[\"response\"]\n",
    "\n",
    "\n",
    "def build_llm_prompt_for_final_symptoms(\n",
    "    conversation: List[Dict[str, Any]],\n",
    "    candidate_symptoms: List[str],\n",
    "    symptom_descriptions: Dict[str, str],\n",
    ") -> tuple[str, str]:\n",
    "    conv_text = build_conversation_block(conversation)\n",
    "\n",
    "    # Build candidate symptom block with short descriptions\n",
    "    lines = []\n",
    "    for name in candidate_symptoms:\n",
    "        desc = symptom_descriptions.get(name, \"\")\n",
    "        if desc:\n",
    "            lines.append(f\"- {name}: {desc}\")\n",
    "        else:\n",
    "            lines.append(f\"- {name}\")\n",
    "    candidates_block = \"\\n\".join(lines)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a medical assistant that identifies which symptoms a patient currently has\n",
    "based on a doctor–patient conversation.\n",
    "\n",
    "You will be given:\n",
    "- A conversation with clear speaker labels \"DOCTOR:\" and \"PATIENT:\".\n",
    "- A list of candidate symptom names (and short descriptions).\n",
    "\n",
    "Rules:\n",
    "- A symptom should be marked PRESENT only if:\n",
    "  1) The PATIENT clearly describes that symptom in their own words, OR\n",
    "  2) The DOCTOR asks directly about that symptom and the PATIENT clearly confirms it\n",
    "     (e.g. \"yes\", \"yeah\", \"a little\", \"sometimes\", or describes it as present).\n",
    "\n",
    "- If the PATIENT clearly denies a symptom (e.g. \"no chest pain\", \"no\", \"not really\"),\n",
    "  do NOT mark that symptom as present.\n",
    "\n",
    "- If the DOCTOR mentions a symptom only as a possibility or question and the patient\n",
    "  does NOT clearly confirm it, do NOT mark it as present.\n",
    "\n",
    "- Focus on current or recent symptoms relevant to this visit.\n",
    "  Ignore remote historical symptoms no longer present.\n",
    "\n",
    "- Use ONLY the candidate symptom names provided.\n",
    "\n",
    "Output:\n",
    "Return ONLY valid JSON of the form:\n",
    "\n",
    "{{\n",
    "  \"symptoms_present\": [\"symptom_name_1\", \"symptom_name_2\", ...]\n",
    "}}\n",
    "\n",
    "Do not add any other keys or commentary.\n",
    "\n",
    "Conversation:\n",
    "\\\"\\\"\\\" \n",
    "{conv_text}\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "Candidate symptoms:\n",
    "{candidates_block}\n",
    "\"\"\"\n",
    "    # return conv_text so we can display & save it\n",
    "    return prompt, conv_text\n",
    "\n",
    "\n",
    "# ==================================================\n",
    "# DISEASE MATRIX HELPERS\n",
    "# ==================================================\n",
    "\n",
    "\n",
    "def collapse_duplicate_diseases(df: pd.DataFrame, disease_col: str = DISEASE_COL) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Combine all rows with the same disease by taking the max across symptom columns.\n",
    "    - If ANY row marks a symptom as 1 → final row gets 1.\n",
    "    \"\"\"\n",
    "    symptom_cols = [c for c in df.columns if c != disease_col]\n",
    "    df_grouped = df.groupby(disease_col)[symptom_cols].max().reset_index()\n",
    "    return df_grouped\n",
    "\n",
    "\n",
    "# Load disease matrix and collapse duplicates\n",
    "disease_df_raw = pd.read_csv(DISEASE_CSV_PATH)\n",
    "disease_df = collapse_duplicate_diseases(disease_df_raw, disease_col=DISEASE_COL)\n",
    "\n",
    "# Identify symptom columns (everything except the disease name)\n",
    "symptom_cols = [c for c in disease_df.columns if c != DISEASE_COL]\n",
    "\n",
    "\n",
    "def rank_diseases_for_symptoms(\n",
    "    symptoms_present,\n",
    "    df: pd.DataFrame = disease_df,\n",
    "    symptom_columns: List[str] | None = None,\n",
    "):\n",
    "    if symptom_columns is None:\n",
    "        symptom_columns = [c for c in df.columns if c != DISEASE_COL]\n",
    "\n",
    "    symptoms_present = list(symptoms_present)\n",
    "\n",
    "    # Only keep symptoms that actually exist as columns\n",
    "    valid_symptoms = [s for s in symptoms_present if s in symptom_columns]\n",
    "\n",
    "    if not valid_symptoms:\n",
    "        raise ValueError(\n",
    "            \"None of the provided symptoms exist as columns in the disease CSV.\"\n",
    "        )\n",
    "\n",
    "    sub = df[valid_symptoms]\n",
    "\n",
    "    # Number of present symptoms that each disease matches\n",
    "    match_count = sub.sum(axis=1)\n",
    "\n",
    "    total_present = len(valid_symptoms)\n",
    "    coverage = match_count / total_present  # fraction of patient symptoms explained\n",
    "\n",
    "    # How many symptoms each disease is associated with in total\n",
    "    disease_symptom_count = df[symptom_columns].sum(axis=1)\n",
    "    # Avoid division by zero\n",
    "    disease_symptom_count_safe = disease_symptom_count.replace(0, np.nan)\n",
    "    precision_like = match_count / disease_symptom_count_safe  # fraction of disease's symptoms present\n",
    "\n",
    "    # Simple combined score\n",
    "    score = 0.7 * coverage + 0.3 * precision_like\n",
    "\n",
    "    result = df[[DISEASE_COL]].copy()\n",
    "    result[\"match_count\"] = match_count\n",
    "    result[\"coverage\"] = coverage\n",
    "    result[\"disease_symptom_count\"] = disease_symptom_count\n",
    "    result[\"precision_like\"] = precision_like\n",
    "    result[\"score\"] = score\n",
    "\n",
    "    # Keep diseases that match at least 1 symptom\n",
    "    result = result[result[\"match_count\"] > 0]\n",
    "\n",
    "    # Sort by score desc, then by match_count desc\n",
    "    result = result.sort_values(by=[\"score\", \"match_count\"], ascending=False).reset_index(drop=True)\n",
    "\n",
    "    return result, valid_symptoms\n",
    "\n",
    "\n",
    "# ==================================================\n",
    "# MAIN PIPELINE\n",
    "# ==================================================\n",
    "\n",
    "\n",
    "def main():\n",
    "    RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    conv_id = CONV_PATH.stem  # e.g. \"conv-3\"\n",
    "\n",
    "    # Create a subfolder for this conversation under results/\n",
    "    conv_results_dir = RESULTS_DIR / conv_id\n",
    "    conv_results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 1) Load symptom index (MiniLM)\n",
    "    index = load_symptom_index(INDEX_DIR)\n",
    "    print(f\"Loaded MiniLM index with {len(index['names'])} symptoms.\")\n",
    "    print(f\"Using embedding model: {index['model_name']}\")\n",
    "\n",
    "    # 2) Load symptom descriptions\n",
    "    symptom_desc_map = load_symptom_descriptions(CSV_PATH)\n",
    "\n",
    "    # 3) Load conversation\n",
    "    conversation = load_conversation(CONV_PATH)\n",
    "    print(f\"Loaded conversation with {len(conversation)} turns.\\n\")\n",
    "\n",
    "    # 4) Build per-turn retrieval DataFrame\n",
    "    df_turns = build_turns_dataframe(conversation, index, top_k=TOP_K_PER_TURN)\n",
    "\n",
    "    print(\"Per-turn candidate symptoms (DataFrame):\")\n",
    "    display(df_turns[[\"turn_idx\", \"speaker\", \"message\", \"top_symptoms\", \"scores\"]])\n",
    "\n",
    "    # 5) Build union of candidate symptoms across all turns\n",
    "    all_candidates: set[str] = set()\n",
    "    for syms in df_turns[\"top_symptoms\"]:\n",
    "        all_candidates.update(syms)\n",
    "\n",
    "    candidate_list = sorted(list(all_candidates))[:MAX_CANDIDATES]\n",
    "\n",
    "    print(\"\\nConcatenated / union list of candidate symptoms (as DataFrame):\")\n",
    "    df_candidates = pd.DataFrame({\"candidate_symptom\": candidate_list})\n",
    "    display(df_candidates)\n",
    "\n",
    "    # 6) Build LLM prompt (also get conv_text for display)\n",
    "    prompt, conv_text = build_llm_prompt_for_final_symptoms(\n",
    "        conversation, candidate_list, symptom_desc_map\n",
    "    )\n",
    "\n",
    "    print(\"\\nFull conversation text passed to LLM:\")\n",
    "    print(\"------------------------------------------------------------\")\n",
    "    print(conv_text)\n",
    "    print(\"------------------------------------------------------------\")\n",
    "\n",
    "    # 7) Call Ollama\n",
    "    print(\"\\nCalling Ollama for final symptom presence...\")\n",
    "    llm_raw = call_ollama(prompt, model=OLLAMA_MODEL)\n",
    "\n",
    "    print(\"\\nRaw LLM response:\")\n",
    "    print(\"------------------------------------------------------------\")\n",
    "    print(llm_raw)\n",
    "    print(\"------------------------------------------------------------\")\n",
    "\n",
    "    # 8) Parse JSON\n",
    "    llm_json = None\n",
    "    try:\n",
    "        llm_json = json.loads(llm_raw)\n",
    "    except json.JSONDecodeError:\n",
    "        try:\n",
    "            start = llm_raw.index(\"{\")\n",
    "            end = llm_raw.rindex(\"}\") + 1\n",
    "            llm_json = json.loads(llm_raw[start:end])\n",
    "        except Exception as e:\n",
    "            print(\"Failed to parse JSON from LLM:\", e)\n",
    "            return\n",
    "\n",
    "    symptoms_present = llm_json.get(\"symptoms_present\", [])\n",
    "\n",
    "    print(\"\\nFinal symptoms present according to LLM (as DataFrame):\")\n",
    "    df_final = pd.DataFrame({\"symptom\": symptoms_present})\n",
    "    display(df_final)\n",
    "\n",
    "    # 9) Rank diseases for these symptoms\n",
    "    symptoms_present_list = df_final[\"symptom\"].tolist()\n",
    "    disease_ranking_df, used_symptoms = rank_diseases_for_symptoms(symptoms_present_list)\n",
    "\n",
    "    print(\"\\nSymptoms used for matching (intersection with CSV columns):\")\n",
    "    print(used_symptoms)\n",
    "\n",
    "    print(\"\\nTop candidate diseases:\")\n",
    "    display(disease_ranking_df.head(20))\n",
    "\n",
    "    # ============ SAVE EVERYTHING TO DISK ============\n",
    "\n",
    "    # To store lists in CSV cleanly, stringify them as JSON\n",
    "    df_turns_for_csv = df_turns.copy()\n",
    "    df_turns_for_csv[\"top_symptoms\"] = df_turns_for_csv[\"top_symptoms\"].apply(json.dumps)\n",
    "    df_turns_for_csv[\"scores\"] = df_turns_for_csv[\"scores\"].apply(json.dumps)\n",
    "\n",
    "    turns_csv_path = conv_results_dir / \"turns.csv\"\n",
    "    candidates_csv_path = conv_results_dir / \"candidates.csv\"\n",
    "    final_csv_path = conv_results_dir / \"final_symptoms.csv\"\n",
    "    disease_csv_path = conv_results_dir / \"disease_ranking.csv\"\n",
    "    llm_ctx_path = conv_results_dir / \"llm_context_and_response.txt\"\n",
    "\n",
    "    df_turns_for_csv.to_csv(turns_csv_path, index=False)\n",
    "    df_candidates.to_csv(candidates_csv_path, index=False)\n",
    "    df_final.to_csv(final_csv_path, index=False)\n",
    "    disease_ranking_df.to_csv(disease_csv_path, index=False)\n",
    "\n",
    "    with open(llm_ctx_path, \"w\") as f:\n",
    "        f.write(\"=== Conversation Text Sent to LLM ===\\n\\n\")\n",
    "        f.write(conv_text)\n",
    "        f.write(\"\\n\\n=== LLM Raw Response ===\\n\\n\")\n",
    "        f.write(llm_raw)\n",
    "\n",
    "    print(\"\\nSaved results to:\")\n",
    "    print(f\"- Per-turn CSV:         {turns_csv_path}\")\n",
    "    print(f\"- Candidates CSV:       {candidates_csv_path}\")\n",
    "    print(f\"- Final symptoms CSV:   {final_csv_path}\")\n",
    "    print(f\"- Disease ranking CSV:  {disease_csv_path}\")\n",
    "    print(f\"- LLM ctx/response TXT: {llm_ctx_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
